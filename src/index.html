<!DOCTYPE html>
<html>
<body>

<h1>Framing Gamification in Undergraduate Cybersecurity Education</h1>

<h1><h1>Sherri Weitl-Harms, Adam Spanier, John Hastings, Matthew Rokusek</h1></h1>

<article>
  <h2>Abstract</h2>
  <p>Gamification    presents    potential    benefits    in    courses   that   traditionally   require   the   comprehension   of   complex  concepts  and  a  high  level  of  technical  and  abstract  thinking.   Courses   in   Cyber   Security   Operations   (CSO)   undergraduate education meet these criterion.This research evaluates organizational constructs that have been   applied   to   gamification   applications   (GAs)   in   CSO   education.   It   utilizes   framing   theory   and   frame-reflective discourse analysis to outline frames based on engagement levels and analyzes the current distribution of GAs.The  following  organizational  constructs  for  GAs  in  data  structures and algorithms education apply to CSO education: Enhanced  Examination  (EE),  Visualization  of  Abstract  Ideas  (VAI),  Dynamic  Gamification  (DG),  Social  and  Collaborative  Engagement     (SGE),     and     Collaborative     Gamification     Development  (CGD).  Three  additional  frames  are  identified:  Missions and Quests (MQ), Simulations (Sim) and Aspirational Learning  (AL).  MQ  GAs  have  process-driven  quests,  stories,  and/or descriptive scenarios to augment engagement. Sim GAs use   environmental   immersion   to   demonstrate   real   world   problem solving while allowing freedom of movement. AL GAs use  goal-based  designs  like  Capture  The  Flag  (CTF)  missions  to enhance engagement.Twenty-seven  existing  CSO  GAs  fit  within  the  MQ  frame  as CSO education lends itself well to these types of experiences. Seventeen  CSO  GAs  fall  within  the  AL  GA  frame,  many  of  these manifesting as CTF missions. Seventeen CSO GAs fit in the  EE  Frame  due  to  their  optimization  in  the  analysis  of  learning progress. Nine Sim GAs were successfully deployed in CSO education, followed by 4 VAI, 3 SGE, and 3 DG GAs..</p>
</article>

<article>
  <h2>Introduction</h2>
  <p>Gamification is defined by Deterding et al. [1] as the use of game elements in non-game environments. This can take a   wide   variety   of   forms,   but   each   exhibits   game-like characteristics  such  as  leaderboards,  badges,  competitive  elements,    cooperation,    communication,    and    advanced    computer imaging [2]. Gamification refers to the use of game elements and game design techniques to augment or improve learning  [3].  Most  significantly,  gamification  as  a  practice  demonstrates  a  notable  increase  in  student  engagement  and  motivation  when  implemented  correctly  [4].  Due  to  this  increase  in  student  engagement,  gamification  finds  itself  at  the  intersection  of  many  fast  growing,  technological  fields,  such as cybersecurity operations (CSO).According    to    the    Bureau    of    Labor    Statistics,    cybersecurity  related  occupations  are  slated  to  increase  as  much as 33% between 2020 and 2030 [5]. As networks get bigger   and   faster,   as   social   media   sites   become   more   comprehensive,  and  as  our  world  becomes  more  digitally  connected, cybercrime and the need for professionals tasked with keeping it at bay will continue to grow at incredible rates [6].  As  it  stands,  the  number  of  CSO  professionals  is  woefully  under  populated.  According  to  the  Information  System Security Association [7], the CSO skills crisis is now 
entering its fifth year, and the outlook isn’t improving.</p>
  
</article>

<article>
  <h2>STUDY DESIGN</h2>
  <p>In  an  effort  to  better  improve  and  streamline  CSO  education  in  gamification  to  meet  industry  demands,  a  framing of existing gamification applications (GAs) used in CSO undergraduate education can provide essential insights into the health and state of gamification in cybersecurity as a whole. This research seeks to organize the comprehensive set of  GAs  evaluated  for  use  in  undergraduate  CSO  education  summarized in [8]. This study also seeks to identify common characteristics  of  GAs  in  undergraduate  CSO  education  for  the purpose of: 1) understanding the value each GA added to the   educational   experience   and   2)   framing   CSO   GAs   accordingly.   To   achieve   these   goals,   we   establish   the   following research questions:•RQ1. Organizational Constructs. What constructs exist that help identify and organize intrinsic characteristics of GAs for CSO education? •RQ2. Characteristics. What characteristics naturally provide order and structure for CSO GAs?•RQ3. Framing. For each CSO GA identified, within which identified frame does it fit?
</p>
  
</article>

<article>
  <h2>ORGANIZATIONAL CONSTRUCTS</h2>
  <p>Organizational  constructs  are  those  structures  that  add  form   and   arrangement   such   that   complex   systems   and   domains    can    be    better    understood.    This    includes    classifications,     taxonomies,     and     other     hierarchical     organizational  structures  [9].  Classifying  and  organizing  items into categories is an important scientific endeavor for describing and understanding related items. For example, in 1735, Linnaeus wrote the Systema Naturae [10], which was indispensable as a foundation zoological nomenclature. On a much  smaller  scale,  this  research  seeks  an  organizational  construct   for   the   set   of   GAs   evaluated   for   use   in   undergraduate CSO education. The intended purpose of RQ1 is  to:  (1)  study  existing  schematic  systems,  (2)  identify  a  system  useful  for  describing  CSO  education  GAs,  and  (3)  modify this system as emergent characteristics appear.A.Classification SystemThere  are  currently  several  classification  systems  based  on game elements. Two such classification system examples are outlined in [11], [12]. According to Werbach and Hunter [11],  the  most  important  game  dynamics  are  constraints,  emotions,  narrative,  progression  and  relationships.  Dicheva  et  al.  [12]  classified  educational  gamification  research  by  game    mechanics,    context    of    applying    gamification,    implementation,  and  evaluation.  During  their  classification  process, [12] primarily used Deterding’s classification [1] of game  design  elements.  They  also  identified  educational  gamification design principles [13] such as goals, challenges and quests, customizations, progress, feedback, etc.Monteiro  et  al.  [14]  created  a  framework  for  evaluating  gamification  systems  in  software  engineering  education.  They  found  that  the  most  common  evaluation  criteria  in  gamification   lies   in   “engagement”,   “motivation”,   and   “satisfaction”.Toda et al. [15] created an element-based taxonomy for classifying   GAs   along   five   dimensions:   performance,   ecological,  social,  personal,  and  fictional.  The  performance  measures are related to the environment response which can be  used  to  provide  feedback  to  the  learner.  Multiple  GAs  were  evaluated  along  these  dimensions,  and  rated  in  each  dimension with a 1-5 rating.Gonzalez et al. [16] developed a classification taxonomy for    cybersecurity    aligned    with    cybersecurity    training materials. Some of the resources described were designed for students;  however,  many  were  not.  Because  of  the  rapidly  evolving  state  of  gamification  in  cybersecurity  education,  many  of  the  resources  listed  are  no  longer  available  while  many  new  applications  have  been  developed.  Similarly,  Chattopadhyay   et   al.   [17]   reviewed   several   popular   cybersecurity   educational   games   as   they   relate   to   the   coverage  of  CSO  curricular  guidelines  [18].  However,  no  classification system was presented in either of these. Petri  and  Wangenheim  [19]  identified  and  evaluated  seven   different   approaches   to   systematically   evaluate   educational  games.  Three  approaches  present  a  framework,  including  a  framework  to  “identify  what  can  potentially  be  evaluated in a GBL application”, a framework to “help tutors to evaluate the potential of using games and simulation-based learning  in  their  practice”,  and  a  framework  to  “assess  the  efficiency of GBL focusing on engineering education”. They identified two approaches that present a scale, one aimed at selecting  good  educational  computer  games  and  another  to  assess   “user   enjoyment   of   e-learning   games   to   help   developers to understand strengths and weaknesses from the students’    perception”.    The    final    approach    was    a    “comprehensive    methodology for    the    research    and    evaluation of serious games”, which “assesses serious games in  three  different  moments  (pre-game,  in-game,  and  post-game).”  While  these  approaches  provide  ways  to  evaluate  aspects   of   individual   GAs,   they   do   not   provide   an   organizational   construct   system   for   ordering   GAs   into   groupings.Carvalho  et  al.  [20]  presented  Activity  Theory-based Model of Serious Games (ATMSG) which has the objective of “supporting the analysis and design of serious games when a thorough understanding of the characteristics of the game is  needed”.  Based  on  ATMSG,  Karagiannis  et  al.  [21],  present the COFELET ontology as a way to describe the key elements that such approaches should embrace to assimilate well  known  cyber  security  threat  analysis  and  modeling  standards  as  the  means  to  create  interesting  educational  experiences. The COFELET ontology was extended with the additional elements of learning objective, grade scheme and role  in  [22].  While  the  ATMSG  model,  the  COFELET  ontology, and similar models, provide good ways to evaluate aspects   of   individual   GAs,   they   do   not   provide   an   organizational   construct   system   for   ordering   GAs   into   groupings.Spanier  et  al.  [23]  reviewed  eight  data  structures  and  algorithms    (DSA)    GAs    and    created    a    systematic    characteristic-based  organizational  construct  for  DSA  GAs.  Rather than using the sum of game element performance as in [11], [12], [23] provided a holistic and qualitative approach to  organization  based  on  emergent  characteristics  of  GAs.  The system presented in [23] consists of five categories: 1) Enhanced  Examination  (EE),  2)  Visualization  of  Abstract  Ideas (VAI), 3) Dynamic Gamification (DG), 4) Social and Collaborative   Engagement   (SCE),   and   5)   Collaborative   Gamification Development (CGD).B.Framing TheoryMayer  [24]  explains  that  providing  clear  definitions  and/or    developing    classifications    or    taxonomies    is    challenging  in  emerging  interdisciplinary  research  areas,  such as gamification, and can “kill innovation because new combinations  cannot  be  boxed”.  Instead  of  classifications, Mayer  [24]  uses  framing  theory  [25]  and  frame-reflective discourse  analysis  [26]  as  a  better  way  to  dissect  how  to  define serious games and the effect they have on the broader discussion  of  the  issue.  Framing  is  the  act  of  attributing  meaning to events and phenomena; a way of creating order out of chaos by providing a critical analysis of the multiple, often conflicting, ways in which we perceive and discuss the utility of games [24]. Frames are defined as definitions of the situation [that] are built up in accordance with the principles 
of organization which govern events—at least social ones—and our subjective involvement in them[25]. Similar to [24]’s framing   of   serious   games,   frame   analysis   is   useful   to   answering RQ1 and providing structure to CSO gamification usage, as it provides a distinction between the interpretation of what is going on while a student is using the CSO GA, and the    interpretation    of    the    phenomena    behind    these experiences</p>
  
</article>

<article>
  <h2>CSO GAMIFICATION FRAMESAND APPLICATIONS</h2>
  <p>To  understand  the  current  state  of  gamification  in  CSO  education,  a  comprehensive  study  of  existing  gamification  implementations  in  CSO  coursework  was  completed  [8].  That study found 74 primary studies that used and evaluated GAs  in  undergraduate  CSO  education.  Some  publications discussed   multiple   GAs,   resulting   in   a   total   of   80   undergraduate CSO GAs to be evaluated.The intended purpose of RQ2 is to understand GAs from a  characteristic-based  point  of  view.  Due  to  the  qualitative  and  emergent  nature  of  RQ2,  the  answer  evolves  as  CSO  gamification applications are discovered and synthesized.Like   the   key   elements   in   the   game-based   learning   evaluation   model   [27],   characteristics   that   are   key   to   formalizing the frames include: the intended purpose of the GA; the level of engagement the student can experience with the  GA;  the  level  of  immersion  the  student  can  experience  within  the  GA;  the  level  of  control  the  player  has  to  manipulate or co-design the game world; the level of social interaction  available  in  the  GA;  and  the  level  of  self-directedness available in the GA.In  this  study,  each  GA  discovered  in  [8]  was  evaluated  and  its  primary  characteristics  identified.  No  judgment  was  made  about  the  quality  and  value  of  the  GAs,  but  the  explanation  provided  for  the  GA  and  its  evaluation  in  undergraduate CSO education were used as a means to place it into a given frame, as an answer to RQ3, as shown in [28]. The  framing  system  used  in  this  paper  is  meaningful  to  explain  how  experiences  with  GAs  in  CSO  undergraduate  education  are  organized. Although  the  frames  are  relative,  they  are  not  irrelevant.  They  structure  ongoing  discourses  about  what  the  GA  can  and  cannot  do  in  terms  of  learning  and change. [24].During this study, several distinct patterns emerged. Like the  DSA  GAs  studied  in  Spanier  et  al.  [23],  1)  several  applications added a gamified interface to a quiz or exercise program,   2)   some   applications   utilized   visualization   to   describe abstract ideas, and 3) some utilized ideas concerning social and collaborative engagement. These patterns offer a ready means to facilitate the evaluation of the organizational constructs   identified   above.   Because   the   patterns   that   emerged  match  those  identified  in  Spanier  et  al.  [23],  the  schema defined in that study is the best answer to RQ1 and provides a starting point for RQ2. This research then applies framing  theory  for  more  flexibility  in  the  organization  and analysis of GAs.Enhanced Examinations (EE) Tests and quizzes by their very nature tend to be tedious and disengaging. EE GAs attempt to better engage students 
2023 Journal of The Colloquium for Information Systems Security Education, Volume 10, No. 1, Winter 2023 979-8-3858-4381-7/23/$ 26.00 ©2023   CISSE4 www.cisse.infowithin  the  context  of  an  exam,  quiz,  or  homework  by  providing a graphically attractive and/or interactive interface [23].  Seventeen  of  the  GAs  discovered  by  [8]  are  listed  on  [28]  as  fitting  the  EE  frame.  For  example,  generalized  education  gamification  frameworks  such  as  Socrative  [29], Kahoot![30], Seppo[31]  and  OneUP  [32],  [33]  have  been  applied  to  CSO  education  [34]  and  thus  fit  withing  the  EE  frame. UltraLearn  [35]  is  a  platform  similar  to  OneUp, designed   to   teach   cybersecurity   to   learners   with   any   background. GamifiedLearn  [36]  is  a  similar  e-learning system.B.Visualization of Abstract Ideas (VAI)Many   CSO   education   GAs   utilize   visualization   to   describe abstract ideas that are difficult to comprehend [23]. Additionally,     visualizations     effectively     and     flexibly     demonstrate  a  step-by-step  walk-through  of  abstract  ideas.  Four undergraduate CSO GAs found in [8] fit within the VAI frame and are listed on [28]. For example, Zhang et al. [37] created  a  web-based  interactive  visualization  tool  that  aims  to  help  students  gain  a  deeper  understanding  of  buffer  overflow  concepts.  It  is  played  as  an  online  game  with  an  analytics dashboard, leaderboards, quizzes, coins and points.C.Missions and Quests (MQ)To enhance engagement, several CSO education GAs add a  story  line  and  well-defined  step-by-step  processes  that  enable  students  to  complete  quests  as  they  progressively  learn  content.  GAs  in  the  MQ  frame  derive  their  main  characteristics from the required steps needed to take to reach the  conclusion.  The  level  of  engagement,  immersion,  and  control that the student can experience with a MQ-based GA is  typically  higher  than  EE-based  GAs,  even  though  these  GAs  still  include  a  means  to  evaluate  student  learning.  For  example, cybersecurity virtual escape rooms provide fun MQ GAs  [38]–[41].  Twenty-seven  of  the  CSO  education  GAs  discovered in [8] are listed in the MQ frame on [28].Representative  MQ  GAs  include  CounterMeasures,  a  series  of  guided  security  missions  [42],  BashDungeon,  an  adventure   inside   a   dungeon,   aimed   at   reproducing   the   topology of a Unix file system [43], Temple of Treasures, a 2D   game   to   learn   Discretionary   Access   Control   and   Mandatory Access Control, where the player is in search of gold,  stuck  in  a  temple,  and  needing  to  gain  knowledge  on  targeted  concepts  to  unlock  the  doors  along  the  escape  pathways  [44],  and  SherLOCKED,  a  2D  top-down  puzzle  adventure game to help students’ knowledge of foundational security concepts [45].D.Simulations (Sim)Simulations    provide    environmental    ambiance    and    context,   oftentimes   via   immersive   content,   into   which   narrative and story are integrated to bolster engagement [46], [47].  In  simulations,  players  are  free  to  move  around  and  explore the environment. Nine CSO education GAs found in [8]  are  listed  as  simulations  in  [28].  For  example,  QuaSim[47] is a 3D GA that poses quantum cryptographic problems developed  by  domain  experts  to  students  who  interactively  move  around  the  environment  to  find  the  solutions.  It  also  facilitates    collaborative    and    competitive    project-based student learning of quantum principles.E.Aspirational Learning (AL)In  CSO  education,  many  educators  make  use  of  goal  driven  simulations,  test-beds  and  competitions  to  augment  student  learning  (  [48],  [49],  [50]).  While  these  CSO  GAs  may  appear  similar  to  MQ-based  or  simulation  GAs,  GAs  that  fit  into  the  AL  frame  are  differentiated  in  that  no  predefined step-based process is required; the student simply needs to accomplish some goal in any way possible as fast as possible. Many of the GAs in the AL frame involve a Capture the Flag competition. Sixteen CSO education GAs found in [8] are listed in the AL frame in [28].For  example,  two  Jeopardy-style  CTFs  were  used  and  evaluated in CSO education in [51]. The CTF competitions consisted of challenges covering several security topics, but did   not   have   a   specific   scenario   or   context   for   the   applications.  Similarly,  a  virtual-machine  (VM)  based  CTF  framework   was   created   by   [52],   for   CSO   students   to   complete Jeopardy-style CTF challenges. They also focused on technical skills and understanding and were not based on a specific scenario. For all exercises, students were required to  submit  written  answers  describing  the  steps  they  took  to  recover  flags  from  the  VM,  and  — where  appropriate  — a description  of  what  the  vulnerabilities  were  and  how  they  worked, and an explanation of how they could be fixed [52].F.Dynamic Gamification (DG)DG  is  defined  as  any  GA  that  dynamically  changes  according to user input throughout its gamified life-cycle. As stated in [23], “DG would still exhibit the same sorts of game mechanics applied in other GAs (e.g., leaderboards, avatars, badges,  awards,  graphical  interfaces,  missions,  objectives, etc.),    but    would    add    a    layer    of    student-led    game    development”.  The  student-led  innovations  within  a  given  game framework provide the dynamic shift in the look, feel, game   mechanics,   and   the   overall   set   of   characteristics   exhibited  by  a  given  gamification   app   implementation;   enabling  students  to  take  ownership  of  the  gamification  experience.DG is a form of discovery learning that follows Bruner’s Constructivism     Theory     [53].     Bruner’s     theory     on     constructivism  encompasses  the  idea  that  “learning  is  an  active  process  in  which  learners  construct  new  ideas  or  concepts   based   upon   their   current/past   experience   or   knowledge”  and  “students  and  instructor  should  engage  in  active  dialog”  [53].  Bruner’s  earlier  work  [54]  established  that  a  good  teacher  will  facilitate  the  learning  process  by  designing lessons that help students discover the relationship between bits of information. DG also provides students with a  software  development  experience  [23].  This  realism  can  help  students,  not  only  learn  the  concepts,  but  also  self-actualize   in   terms   of   seeing   themselves   as   software   developers [23].Three   CSO   GAs   discovered   in   [8]   exhibit   DG   characteristics as listed in [28]. For example, in [55], students participate  in  a  game-development  based  learning  project</p></article>

<article>
  <h2>References</h2>
  <p>S. Deterding, D. Dixon, R. Khaled, and L. Nacke, “From game design elements to gamefulness: defining “gamification,” in Proceedings of the 15th international academic MindTrek conference: Envisioning future media environments, 2011. doi: 10.1145/2181037.2181040 pp. 9–15.[2]S. Villagrasa, D. Fonseca, E. Redondo, and J. Duran, “Teaching case of gamification and visual technologies for education,” Journal of Cases on Information Technology (JCIT), vol. 16, no. 4. doi: 10.4018/jcit.2014100104 pp. 38–57, 2014.[3]M. R. d. A. Souza, L. F. Veado, R. R. Moreira, E. M. L. Figueiredo, and H. Costa, “A systematic mapping study on game-related methods for software engineering education,” Information and Software Technology, vol. 95, pp. 201–218, 2018.[4]S. Sandusky, “Gamification in education,” 2015. [Online]. Available: http://hdl.handle.net/10150/556222[5]Bureau of Labor and Statistics, “Information security analyst,” 2022, accessed: 10-Mar-2022. [Online]. Available: https://www.bls.gov/ooh/computer-and-information-technology/information-security-analysts.htm[6]T. Riley, “The cybersecurity 202: Cybercrime skyrocketed as workplaces went virtual in 2020, new report finds,” 2021, accessed: 10-Mar-2022. [Online]. Available: https://www.washingtonpost.com/politics/2021/02/22/cybersecurity-202-cybercrime-skyrocketed-workplaces-went-virtual-2020[7]I. S. S. Association, “Cybersecurity skills crisis continues for fifth year, perpetuated by lack of business investment,” 2021, accessed: 10-Mar-2022. [Online]. Available: https://www.issa.org/cybersecurity-skills-crisis-continues-for-fifth-year-perpetuated-by-lack-of-business-investment [8]S. K. Weitl-Harms, A. M. Spanier, J. D. Hastings, and M. Rokusek, “A systematic mapping study on gamification applications for undergraduate cybersecurity education,” Journal on Cybersecurity Education, Research, and Practice, 2022.[9]W. B. Wolf, “Organizational constructs: An approach to understanding organizations,” Academy of Management Journal, vol. 1, no. 1. pp. 7–15, 1958.[10]C. Linnaeus, Systema Naturae. Lugduni Batavorum. (Haak), 1735.[11]K. Werback and D. Hunter, The Gamification Toolkit. Philadelphia, PA: Wharton School Press, 2015.[12]D. Dicheva, C. Dichev, G. Agre, and G. Angelova, “Gamification in education: A systematic mapping study,” Journal of Educational technology & society, vol. 18, no. 3, pp. 75–88, 2015.[13]S. Björk and J. Holopainen, Patterns in Game Design. Boston, MA: Charles River Media, Boston, MA, 2005.[14]R. H. B. Monteiro, S. R. B. Oliveira, and M. R. De Almeida Souza, “A standard framework for gamification evaluation in education and training of software engineering: an evaluation from a proof of concept,” in IEEE Frontiers in Education Conference (FIE), Lincoln, NE, 2021. doi: 10.1109/FIE49875.2021.9637232 pp. 1–7. [15]A. Toda, A. Klock, W. Oliveira, P. Palomino, L. Rodrigues, L. Shi, I. Bittencourt, I. Gasparini, S. Isotani, and A. Cristea, “Analysing gamification elements in educational environments using an existing gamification taxonomy,” Smart Learning Environments, vol. 6, no. 1, pp. 1–14, 2019.[16]H. Gonzalez, R. Llamas, and F. Ordaz, “Cybersecurity teaching through gamification: aligning training resources to our syllabus,” Research in Computer Science, vol. 146. doi: 10.13053/RCS-146-1-4 pp. 35–43, 2017.[17]A. Chattopadhyay, C. Maschinot, and L. Nestor, “Mirror mirror on the wall - what are cybersecurity educational games offering overall: A research study and gap analysis,” in 2021 IEEE Frontiers in Education Conference (FIE). Lincoln, NE: IEEE, 2021. doi: 10.1109/FIE49875.2021.9637224 pp. 1–8. [18]ACM-IEEE Joint Task Force on CyberSecurity Curricula, “Cybersecurity science curricula 2017: Curriculum guidelines for postsecondary degree programs in cybersecurity,” 2017. [Online]. Available: https://www.acm.org/binaries/content/assets/education/curricula-recommendations/csec2017.pdf[19]G. Petri and C. G. von Wangenheim, “How to evaluate educational games: a systematic literature review,” JUCS - Journal of Universal Computer Science, vol. 22, no. 7. doi: 10.3217/jucs-022-07-0992 pp. 992–1021, 2016.[20]M. B. Carvalho, F. Bellotti, R. Berta, A. De Gloria, C. I. Sedano, J. B. Hauge, J. Hu, and M. Rauterberg, “An activity theory-based model for serious games analysis and conceptual design,” Computers & Education, vol. 87. doi: 10.1016/j.compedu.2015.03.023 pp. 166–181, 2015.[21]S. Karagiannis and E. Magkos, “Adapting CTF challenges into virtual cybersecurity learning environments,” Information & Computer Security, vol. 29, no. 1. doi: 10.1108/ICS-04-2019-0050 pp. 105–132, 2021.[22]M. N. Katsantonis, I. Mavridis, and D. Gritzalis, “Design and evaluation of cofelet-based approaches for cyber security learning and training,” Computers & Security, vol. 105. doi: 10.1016/J.COSE.2021.102263 p. 102263, 2021.[23]A. M. Spanier, S. K. Weitl-Harms, and J. D. Hastings, “A classification scheme for gamification in computer science education: Discovery of foundational gamification genres in data structures courses,” in IEEE Frontiers in Education Conference (FIE), Lincoln, NE, 2021. doi: 10.1109/FIE49875.2021.9637447 pp. 1–9. [24]I. Mayer, H. Warmelink, and Q. Zhou, “A frame-reflective discourse analysis of serious games,” British Journal of Educational Technology, vol. 47, no. 2. doi: 10.1111/bjet.12245 pp. 342–357, 2016.[25]E. Goffman, Frame Analysis: An Essay on the Organization of Experience. Harper & Row, 1974.[26]M. Rein and D. A. Scho ̈n, “Frame-critical policy analysis and frame-reflective policy practice,” Knowledge and Policy, vol. 9, no. 1. doi: 10.1007/BF02832235 p. 85–104, 1996.[27]E. Oprins, G. Visschedijk, M. B. Roozeboom, M. Dankbaar, W. Trooster, and S. C. Schuit, “The game-based learning evaluation model (GEM): measuring the effectiveness of serious games using a standardised method,” International Journal of Technology Enhanced Learning, vol. 7, no. 4. doi: 10.1504/IJTEL.2015.074189 pp. 326–345, 2015.[28]S. K. Weitl-Harms, A. M. Spanier, J. D. Hastings, and M. Rokusek, “CSO gamification applications listing 2022,” 2022. [Online]. Available: https://bit.ly/3S260GS[29]S. Inc., “Sacrative,” 2022, accessed: 14-Mar-2022. [Online]. Available: https://www.socrative.com[30]Kahoot!, “Kahoot!” 2022, accessed: 14-Mar-2022. [Online]. Available: https://kahoot.it[31]Seppo, “Seppo,” 2021, accessed: 14-Mar-2022. [Online]. Available: http://seppo.io[32]F. Demmese, X. Yuan, and D. Dicheva, “Evaluating the effectiveness of gamification on students’ performance in a cybersecurity course,” Journal of the Colloquium for Information System Security Education, vol. 8, no. 1, pp. 1–12, 2020. [Online]. Available: https://par.nsf.gov/biblio/10290874[33]D. Dicheva, K. Irwin, and C. Dichev, “OneUp learning: A course gamification platform,” in Games and Learning Alliance (GALA). 
</p>
</article>
<h2>addendum</h2>
<p>!DOCTYPE html: This is a declaration that specifies the document type and version of HTML being used, which is HTML5 in this case. It helps browsers interpret the document correctly.</p>
<p>html: This tag represents the root element of an HTML document. All other elements are nested within this tag.</p>
<p>body: The body tag defines the main content of the HTML document that is visible to the user. It typically contains text, headings, paragraphs, links, and other elements that make up the web page's content.</p>
<p>h1: This is a heading tag, and it defines the top-level heading or title of a section or the entire page. In the provided document, it's used for the main title and author names.</p>
<p>article: The article tag is used to define a self-contained composition within a document, such as a blog post, news article, or any content that can stand alone.</p>
<p>p: The p tag is used to define a paragraph of text. It separates blocks of text into individual paragraphs.</p>
</body>
</html>
